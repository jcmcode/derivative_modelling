{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ae41ae",
   "metadata": {},
   "source": "# Breeden-Litzenberger Model Implementation\n# Extracting Risk-Neutral Probability Distributions from Option Prices\n\nThis notebook implements the **Breeden-Litzenberger (1978)** method to recover the\nrisk-neutral probability density (RND) implied by market option prices on SPY (S&P 500 ETF).\n\n**Pipeline overview:**\n1. Fetch live option chain data for SPY via yfinance\n2. Clean and filter for liquid, near-term options (~30 days to expiry)\n3. Compute implied volatilities and construct the volatility smile\n4. Interpolate/extrapolate the smile using a clamped cubic spline (Malz 2014)\n5. Build a fine-grid call price function via Black-Scholes with the interpolated vols\n6. Apply Breeden-Litzenberger finite differencing to extract the RND\n7. Compare the market-implied distribution against the theoretical lognormal (Black-Scholes)\n8. Analyse tail risk, skewness, and kurtosis"
  },
  {
   "cell_type": "markdown",
   "id": "69347ade",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61266aab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom scipy.interpolate import CubicSpline\nfrom scipy.stats import norm, lognorm\nfrom scipy.optimize import brentq\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Compatibility: numpy >= 2.0 renamed trapz -> trapezoid\ntrapz = np.trapezoid if hasattr(np, 'trapezoid') else np.trapz\n\nprint(\"All imports loaded successfully.\")"
  },
  {
   "cell_type": "markdown",
   "id": "e149f4ee",
   "metadata": {},
   "source": "## Black-Scholes Pricing Functions\n\nCore pricing engine used throughout. The BS formula gives us:\n- A way to **invert** market prices into implied volatilities\n- A way to **reconstruct** call prices from interpolated vols on a fine strike grid\n- The **theoretical lognormal density** to compare against the market-implied RND"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13a15d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "def bs_d1(S, K, T, r, q, sigma):\n    \"\"\"Compute Black-Scholes d1 parameter.\"\"\"\n    return (np.log(S / K) + (r - q + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n\ndef bs_d2(S, K, T, r, q, sigma):\n    \"\"\"Compute Black-Scholes d2 parameter.\"\"\"\n    return bs_d1(S, K, T, r, q, sigma) - sigma * np.sqrt(T)\n\ndef bs_call_price(S, K, T, r, q, sigma):\n    \"\"\"\n    Black-Scholes European call price.\n    \n    Parameters:\n        S     : Current underlying price\n        K     : Strike price (scalar or array)\n        T     : Time to expiration (years)\n        r     : Risk-free rate (continuously compounded)\n        q     : Dividend yield (continuously compounded)\n        sigma : Volatility (scalar or array matching K)\n    \n    Returns:\n        Call option price(s)\n    \"\"\"\n    d1 = bs_d1(S, K, T, r, q, sigma)\n    d2 = d1 - sigma * np.sqrt(T)\n    return S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\ndef bs_put_price(S, K, T, r, q, sigma):\n    \"\"\"Black-Scholes European put price via put-call parity.\"\"\"\n    return bs_call_price(S, K, T, r, q, sigma) - S * np.exp(-q * T) + K * np.exp(-r * T)\n\ndef bs_vega(S, K, T, r, q, sigma):\n    \"\"\"Black-Scholes vega (sensitivity of price to volatility).\"\"\"\n    d1 = bs_d1(S, K, T, r, q, sigma)\n    return S * np.exp(-q * T) * norm.pdf(d1) * np.sqrt(T)\n\ndef implied_vol_call(market_price, S, K, T, r, q, lower=0.001, upper=5.0):\n    \"\"\"\n    Invert the BS formula to find implied volatility from a market call price.\n    Uses Brent's method (guaranteed convergence for bracketed roots).\n    \"\"\"\n    intrinsic = max(S * np.exp(-q * T) - K * np.exp(-r * T), 0)\n    if market_price <= intrinsic + 1e-10:\n        return np.nan\n    \n    def objective(sigma):\n        return bs_call_price(S, K, T, r, q, sigma) - market_price\n    \n    try:\n        if objective(lower) * objective(upper) > 0:\n            return np.nan\n        return brentq(objective, lower, upper, xtol=1e-12, maxiter=200)\n    except (ValueError, RuntimeError):\n        return np.nan\n\ndef implied_vol_put(market_price, S, K, T, r, q, lower=0.001, upper=5.0):\n    \"\"\"Invert BS for implied vol from a market put price.\"\"\"\n    # Convert to call price via put-call parity, then solve\n    call_price = market_price + S * np.exp(-q * T) - K * np.exp(-r * T)\n    return implied_vol_call(call_price, S, K, T, r, q, lower, upper)\n\nprint(\"Black-Scholes pricing functions defined.\")"
  },
  {
   "cell_type": "markdown",
   "id": "44520970",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": "## Step 1: Fetch Market Data"
  },
  {
   "cell_type": "code",
   "id": "ytfnoeu5jer",
   "source": "# --- Configuration ---\nTICKER = \"SPY\"\nTARGET_DTE = 30  # Target ~30 days to expiry\n\n# Market parameters (approximate current values)\nRISK_FREE_RATE = 0.043   # ~4.3% (3-month T-bill yield)\nDIVIDEND_YIELD = 0.013   # ~1.3% (SPY trailing dividend yield)\n\n# --- Fetch underlying price ---\nspy = yf.Ticker(TICKER)\nhist = spy.history(period=\"5d\")\nS0 = hist['Close'].iloc[-1]\nprint(f\"Underlying: {TICKER}\")\nprint(f\"Current price (S0): ${S0:.2f}\")\n\n# --- Select the nearest expiry to our target DTE ---\nexpiry_dates = spy.options\nprint(f\"\\nAvailable expiries: {len(expiry_dates)}\")\n\nfrom datetime import datetime, timedelta\ntoday = datetime.today()\ntarget_date = today + timedelta(days=TARGET_DTE)\n\n# Find expiry closest to target\nexpiry_dtes = []\nfor exp_str in expiry_dates:\n    exp_date = datetime.strptime(exp_str, \"%Y-%m-%d\")\n    dte = (exp_date - today).days\n    expiry_dtes.append((exp_str, dte, abs(dte - TARGET_DTE)))\n\nexpiry_dtes.sort(key=lambda x: x[2])\nselected_expiry = expiry_dtes[0][0]\nactual_dte = expiry_dtes[0][1]\nT = actual_dte / 365.0  # Time to expiry in years\n\nprint(f\"Selected expiry: {selected_expiry} ({actual_dte} DTE)\")\nprint(f\"T = {T:.4f} years\")\n\n# --- Fetch option chain ---\nchain = spy.option_chain(selected_expiry)\ncalls_raw = chain.calls.copy()\nputs_raw = chain.puts.copy()\n\nprint(f\"\\nRaw calls: {len(calls_raw)} strikes\")\nprint(f\"Raw puts:  {len(puts_raw)} strikes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "80cakb9shp",
   "source": "## Step 2: Clean and Filter Option Data\n\nWe need to:\n- Use **mid prices** (average of bid/ask) to reduce microstructure noise\n- Filter out illiquid options (zero bid, wide spreads, low volume/OI)\n- Focus on strikes within a reasonable range around ATM (moneyness 0.7-1.3)\n- Use **OTM options only**: OTM puts for strikes below ATM, OTM calls for strikes above ATM  \n  (OTM options are more liquid and have tighter spreads than their ITM counterparts)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n1hx8xz9n2i",
   "source": "def clean_options(df, option_type, S, min_price=0.05, min_oi=10, moneyness_range=(0.70, 1.30)):\n    \"\"\"\n    Clean and filter option data.\n    \n    Handles both live market (bid/ask available) and after-hours (lastPrice only).\n    \"\"\"\n    df = df.copy()\n    \n    # Use mid price if bid/ask are available, otherwise fall back to lastPrice\n    has_quotes = (df['bid'] > 0) & (df['ask'] > 0)\n    df['mid'] = np.where(has_quotes, (df['bid'] + df['ask']) / 2, df['lastPrice'])\n    df['spread'] = np.where(has_quotes, df['ask'] - df['bid'], np.nan)\n    df['moneyness'] = df['strike'] / S\n    df['has_live_quotes'] = has_quotes\n    \n    # Filter\n    mask = (\n        (df['mid'] >= min_price) &\n        (df['openInterest'] >= min_oi) &\n        (df['moneyness'] >= moneyness_range[0]) &\n        (df['moneyness'] <= moneyness_range[1])\n    )\n    df = df[mask].copy()\n    \n    return df[['strike', 'bid', 'ask', 'mid', 'spread', 'lastPrice', 'volume', \n               'openInterest', 'impliedVolatility', 'moneyness', 'has_live_quotes']].sort_values('strike')\n\n# Clean both chains\ncalls = clean_options(calls_raw, 'call', S0)\nputs = clean_options(puts_raw, 'put', S0)\n\nquote_status = \"LIVE bid/ask\" if calls['has_live_quotes'].any() else \"lastPrice (market closed)\"\nprint(f\"Price source: {quote_status}\")\nprint(f\"Filtered calls: {len(calls)} strikes\")\nprint(f\"Filtered puts:  {len(puts)} strikes\")\nif len(calls) > 0:\n    print(f\"\\nCalls strike range: ${calls['strike'].min():.0f} - ${calls['strike'].max():.0f}\")\nif len(puts) > 0:\n    print(f\"Puts strike range:  ${puts['strike'].min():.0f} - ${puts['strike'].max():.0f}\")\nprint(f\"ATM strike ~ ${S0:.0f}\")\n\n# Preview\nprint(\"\\n--- Sample Calls (near ATM) ---\")\natm_mask = (calls['moneyness'] > 0.97) & (calls['moneyness'] < 1.03)\nprint(calls[atm_mask][['strike', 'mid', 'openInterest', 'moneyness']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dk5lldmtt3e",
   "source": "## Step 3: Compute Implied Volatilities and Build the Volatility Smile\n\nWe compute IV by **inverting the Black-Scholes formula** using Brent's root-finding method.\n\nFor the smile construction, we follow market convention:\n- Use **OTM puts** for K < S (lower strikes) and **OTM calls** for K > S (upper strikes)\n- At-the-money, we average both\n\nThis gives us the cleanest implied volatility surface since OTM options are more liquid.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "75cuw6mrdsg",
   "source": "r = RISK_FREE_RATE\nq = DIVIDEND_YIELD\n\n# Compute implied vols from market mid prices\ncall_ivs = []\nfor _, row in calls.iterrows():\n    iv = implied_vol_call(row['mid'], S0, row['strike'], T, r, q)\n    call_ivs.append(iv)\ncalls = calls.copy()\ncalls['iv_computed'] = call_ivs\n\nput_ivs = []\nfor _, row in puts.iterrows():\n    iv = implied_vol_put(row['mid'], S0, row['strike'], T, r, q)\n    put_ivs.append(iv)\nputs = puts.copy()\nputs['iv_computed'] = put_ivs\n\n# Drop failed IV computations\ncalls = calls.dropna(subset=['iv_computed'])\nputs = puts.dropna(subset=['iv_computed'])\n\nprint(f\"Valid IVs: {len(calls)} calls, {len(puts)} puts\")\n\n# Build combined smile: OTM puts (K < S0) + OTM calls (K >= S0)\notm_puts = puts[puts['strike'] < S0][['strike', 'iv_computed', 'moneyness']].copy()\notm_puts = otm_puts.rename(columns={'iv_computed': 'iv'})\notm_puts['source'] = 'OTM Put'\n\notm_calls = calls[calls['strike'] >= S0][['strike', 'iv_computed', 'moneyness']].copy()\notm_calls = otm_calls.rename(columns={'iv_computed': 'iv'})\notm_calls['source'] = 'OTM Call'\n\nsmile = pd.concat([otm_puts, otm_calls], ignore_index=True).sort_values('strike')\nsmile = smile.drop_duplicates(subset='strike', keep='first')\n\n# Remove any extreme outliers (IV > 1.5 or < 0.01)\nsmile = smile[(smile['iv'] > 0.01) & (smile['iv'] < 1.5)]\n\nprint(f\"Volatility smile: {len(smile)} data points\")\nif len(smile) > 0:\n    print(f\"Strike range: ${smile['strike'].min():.0f} - ${smile['strike'].max():.0f}\")\n    print(f\"IV range: {smile['iv'].min():.4f} - {smile['iv'].max():.4f}\")\n    atm_idx = smile['moneyness'].sub(1).abs().idxmin()\n    print(f\"ATM IV ~ {smile.loc[atm_idx, 'iv']:.4f} ({smile.loc[atm_idx, 'iv']*100:.2f}%)\")\nelse:\n    print(\"WARNING: No valid smile data points. Check data source.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ghr51ztfzo",
   "source": "# --- Plot the raw volatility smile ---\nfig = go.Figure()\n\n# Color by source\nfor source, color in [('OTM Put', '#EF553B'), ('OTM Call', '#636EFA')]:\n    mask = smile['source'] == source\n    fig.add_trace(go.Scatter(\n        x=smile[mask]['moneyness'],\n        y=smile[mask]['iv'] * 100,\n        mode='markers',\n        name=source,\n        marker=dict(size=7, color=color),\n        hovertemplate='K=$%{customdata[0]:.0f}<br>Moneyness=%{x:.3f}<br>IV=%{y:.2f}%',\n        customdata=smile[mask][['strike']].values\n    ))\n\nfig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"ATM\")\nfig.update_layout(\n    title=f\"Implied Volatility Smile - {TICKER} ({selected_expiry}, {actual_dte} DTE)\",\n    xaxis_title=\"Moneyness (K/S)\",\n    yaxis_title=\"Implied Volatility (%)\",\n    template=\"plotly_white\",\n    width=900, height=500,\n    legend=dict(x=0.02, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tj7ko217lye",
   "source": "## Step 4: Interpolate and Extrapolate the Volatility Smile\n\nFollowing **Malz (2014, Sec. 2.1\u20132.2)**, we fit a **clamped cubic spline** to the implied volatility data:\n\n- **Clamped** means the first derivative is set to zero at the boundary knots (Malz 2014, Eq. 5\u20136). This ensures \n  flat extrapolation beyond the observed strikes, which prevents no-arbitrage violations \n  (the call valuation function stays monotonically decreasing and convex).\n- **Cubic spline** ensures C\u00b2 continuity (continuous first and second derivatives), which is \n  essential since the BL method requires the second derivative of call prices \n  (Breeden & Litzenberger 1978, Eq. 2; Jackwerth 2004, Eq. 7).\n- Beyond observed strikes, we extrapolate at constant IV (flat wings).\n\nThe resulting **call valuation function** is defined as (Malz 2014, Eq. 4):\n\n$$c(t, X, \\tau) = v[S_t, X, \\tau, \\sigma(X), r, q]$$\n\nwhere $v[\\cdot]$ is the Black-Scholes formula and $\\sigma(X)$ is the interpolated smile.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "c21xz3otz3c",
   "source": "# --- Fit clamped cubic spline to the volatility smile ---\nstrikes_data = smile['strike'].values\nivs_data = smile['iv'].values\n\n# Clamped cubic spline: first derivative = 0 at boundaries (flat extrapolation)\n# bc_type=((1, 0), (1, 0)) means first derivative = 0 at both endpoints\nvol_spline = CubicSpline(strikes_data, ivs_data, bc_type=((1, 0.0), (1, 0.0)))\n\n# Define the interpolated/extrapolated vol function with flat wings\nK_min_data = strikes_data[0]\nK_max_data = strikes_data[-1]\niv_min = ivs_data[0]\niv_max = ivs_data[-1]\n\ndef sigma_interp(K):\n    \"\"\"\n    Interpolated implied volatility function.\n    - Inside observed range: clamped cubic spline\n    - Outside observed range: flat extrapolation at boundary IV\n    \"\"\"\n    K = np.atleast_1d(K)\n    result = np.empty_like(K, dtype=float)\n    \n    below = K < K_min_data\n    above = K > K_max_data\n    inside = ~below & ~above\n    \n    result[below] = iv_min\n    result[above] = iv_max\n    result[inside] = vol_spline(K[inside])\n    \n    # Ensure IV is always positive\n    result = np.maximum(result, 0.001)\n    \n    return result\n\n# --- Build the call valuation function on a fine grid ---\n# This is the key: c(t, X, tau) = v(S, X, tau, sigma(X), r, q)\n# where sigma(X) comes from the interpolated smile\n\n# Fine grid extending well beyond observed strikes\nF = S0 * np.exp((r - q) * T)  # Forward price\nK_low = S0 * 0.50    # 50% of spot\nK_high = S0 * 1.50   # 150% of spot\nN_grid = 2000\nK_fine = np.linspace(K_low, K_high, N_grid)\n\n# Compute interpolated vols and call prices on fine grid\nsigma_fine = sigma_interp(K_fine)\nC_fine = bs_call_price(S0, K_fine, T, r, q, sigma_fine)\n\nprint(f\"Forward price: F = ${F:.2f}\")\nprint(f\"Fine grid: {N_grid} points from ${K_low:.0f} to ${K_high:.0f}\")\nprint(f\"Grid spacing: dK = ${(K_high - K_low) / N_grid:.4f}\")\n\n# --- Plot: Interpolated smile overlaid on raw data ---\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=smile['strike'], y=smile['iv'] * 100,\n    mode='markers', name='Market IV (raw)',\n    marker=dict(size=7, color='#EF553B')\n))\n\nfig.add_trace(go.Scatter(\n    x=K_fine, y=sigma_fine * 100,\n    mode='lines', name='Clamped Cubic Spline',\n    line=dict(color='#636EFA', width=2)\n))\n\nfig.add_vline(x=S0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Spot\")\nfig.add_vline(x=K_min_data, line_dash=\"dot\", line_color=\"orange\", annotation_text=\"Data min\")\nfig.add_vline(x=K_max_data, line_dash=\"dot\", line_color=\"orange\", annotation_text=\"Data max\")\n\nfig.update_layout(\n    title=f\"Volatility Smile: Raw Data vs Clamped Cubic Spline\",\n    xaxis_title=\"Strike Price ($)\",\n    yaxis_title=\"Implied Volatility (%)\",\n    template=\"plotly_white\",\n    width=900, height=500\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w3oek93bcob",
   "source": "# --- Plot the call valuation function ---\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=K_fine, y=C_fine,\n    mode='lines', name='Call Price c(K)',\n    line=dict(color='#636EFA', width=2)\n))\n\n# Overlay actual market mid prices for calls\nfig.add_trace(go.Scatter(\n    x=calls['strike'], y=calls['mid'],\n    mode='markers', name='Market Call Prices',\n    marker=dict(size=5, color='#EF553B')\n))\n\nfig.add_vline(x=S0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Spot\")\nfig.update_layout(\n    title=\"Call Valuation Function c(K) from Interpolated Smile\",\n    xaxis_title=\"Strike Price ($)\",\n    yaxis_title=\"Call Price ($)\",\n    template=\"plotly_white\",\n    width=900, height=500\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lrmuhrzohf9",
   "source": "## Step 5: Breeden-Litzenberger \u2014 Extract the Risk-Neutral Distribution\n\nThis is the core result. From the **Breeden-Litzenberger (1978) Theorem 1**, also presented as **Jackwerth (2004, Eq. 7)** and **Malz (2014, Eq. 2\u20133)**:\n\n**Risk-neutral CDF** (Malz 2014, Eq. 2):\n\n$$\\tilde{\\Pi}_t(X) = 1 + e^{r\\tau} \\frac{\\partial c}{\\partial X}$$\n\n**Risk-neutral PDF** (Breeden & Litzenberger 1978, Eq. 2; Malz 2014, Eq. 3):\n\n$$\\tilde{\\pi}_t(X) = e^{r\\tau} \\frac{\\partial^2 c}{\\partial X^2}$$\n\nWe approximate these with **central finite differences** (Malz 2014, Sec. 2.3):\n\n$$\\tilde{\\Pi}_t(X) \\approx 1 + e^{r\\tau} \\frac{1}{\\Delta}\\left[c\\left(X + \\tfrac{\\Delta}{2}\\right) - c\\left(X - \\tfrac{\\Delta}{2}\\right)\\right]$$\n\n$$\\tilde{\\pi}_t(X) \\approx e^{r\\tau} \\frac{1}{\\Delta^2}\\left[c(X + \\Delta) + c(X - \\Delta) - 2\\,c(X)\\right]$$\n\nThe second formula is the **butterfly spread** decomposition \u2014 in the limit $\\Delta \\to 0$, the butterfly payoff converges to an Arrow-Debreu security (Breeden & Litzenberger 1978, Theorem 1).\n\nThe step size $\\Delta$ is a smoothing parameter (Malz 2014, Sec. 2.4) \u2014 too small and noise dominates, too large and we lose resolution. We use $\\Delta = 0.025 \\times F$.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oxmfsanupa",
   "source": "def call_value(K_arr):\n    \"\"\"\n    Compute BS call prices at arbitrary strikes using the interpolated vol smile.\n    This is the call valuation function c(t, X, tau) from Malz (2014) eq. (4).\n    \"\"\"\n    K_arr = np.atleast_1d(K_arr)\n    sigmas = sigma_interp(K_arr)\n    return bs_call_price(S0, K_arr, T, r, q, sigmas)\n\ndef breeden_litzenberger(K_grid, delta_frac=0.025):\n    \"\"\"\n    Apply the Breeden-Litzenberger method to extract:\n    - Risk-neutral CDF (cumulative distribution)\n    - Risk-neutral PDF (probability density)\n    \n    Parameters:\n        K_grid     : Array of strike prices to evaluate\n        delta_frac : Step size as fraction of forward price (Malz 2014 recommends ~0.025)\n    \n    Returns:\n        K_grid : Strike prices\n        cdf    : Risk-neutral CDF at each strike\n        pdf    : Risk-neutral PDF at each strike\n    \"\"\"\n    Delta = delta_frac * F  # Absolute step size in dollars\n    discount = np.exp(r * T)  # Future value factor\n    \n    # --- CDF via first derivative (central difference) ---\n    c_up_half = call_value(K_grid + Delta / 2)\n    c_dn_half = call_value(K_grid - Delta / 2)\n    cdf = 1.0 + discount * (c_up_half - c_dn_half) / Delta\n    \n    # --- PDF via second derivative (central difference) ---\n    c_center = call_value(K_grid)\n    c_up = call_value(K_grid + Delta)\n    c_dn = call_value(K_grid - Delta)\n    pdf = discount * (c_up + c_dn - 2.0 * c_center) / (Delta**2)\n    \n    return K_grid, cdf, pdf\n\n# --- Compute the RND ---\nK_rnd = np.linspace(S0 * 0.70, S0 * 1.30, 1500)\n_, rn_cdf, rn_pdf = breeden_litzenberger(K_rnd, delta_frac=0.025)\n\n# Ensure non-negative densities (numerical artifact cleanup)\nrn_pdf = np.maximum(rn_pdf, 0)\n\n# Normalise the PDF so it integrates to 1 (numerical correction)\ntotal_mass = trapz(rn_pdf, K_rnd)\nrn_pdf_norm = rn_pdf / total_mass\n\nprint(f\"Step size Delta = {0.025 * F:.2f} (= 2.5% of forward ${F:.2f})\")\nprint(f\"PDF total mass before normalisation: {total_mass:.6f}\")\nprint(f\"PDF total mass after normalisation:  {trapz(rn_pdf_norm, K_rnd):.6f}\")\nprint(f\"CDF range: [{rn_cdf.min():.4f}, {rn_cdf.max():.4f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "211kfnbrckq",
   "source": "## Step 6: Lognormal Benchmark (Black-Scholes Theoretical Distribution)\n\nUnder Black-Scholes assumptions, the risk-neutral distribution of $S_T$ is **lognormal**:\n\n$$\\ln S_T \\sim \\mathcal{N}\\left(\\ln S_0 + (r - q - \\tfrac{\\sigma^2}{2})T,\\;\\; \\sigma^2 T\\right)$$\n\nWe use the ATM implied volatility as $\\sigma$ to build this benchmark. Any deviation between \nthe market-implied RND and this lognormal represents the market's view that returns are \n**not** normally distributed \u2014 revealing skew, excess kurtosis, and tail risk.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "55tch6q3zc4",
   "source": "# --- Lognormal benchmark from Black-Scholes ---\n# ATM implied vol\nsigma_atm = float(sigma_interp(np.array([S0]))[0])\nprint(f\"ATM implied volatility: {sigma_atm:.4f} ({sigma_atm*100:.2f}%)\")\n\n# Under risk-neutral measure, ln(S_T) ~ N(mu_ln, sigma_ln^2)\nmu_ln = np.log(S0) + (r - q - 0.5 * sigma_atm**2) * T\nsigma_ln = sigma_atm * np.sqrt(T)\n\n# Lognormal PDF: f(x) = (1/(x * sigma_ln * sqrt(2*pi))) * exp(-(ln(x) - mu_ln)^2 / (2*sigma_ln^2))\nlognorm_pdf = (1.0 / (K_rnd * sigma_ln * np.sqrt(2 * np.pi))) * \\\n              np.exp(-0.5 * ((np.log(K_rnd) - mu_ln) / sigma_ln)**2)\n\n# Lognormal CDF\nlognorm_cdf = norm.cdf((np.log(K_rnd) - mu_ln) / sigma_ln)\n\nprint(f\"Lognormal mean (E[S_T]): ${np.exp(mu_ln + 0.5*sigma_ln**2):.2f}\")\nprint(f\"Forward price:           ${F:.2f}\")\nprint(f\"Lognormal std dev:       ${np.exp(mu_ln + 0.5*sigma_ln**2) * np.sqrt(np.exp(sigma_ln**2) - 1):.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cuzjljxq1is",
   "source": "# =============================================================================\n# MAIN RESULT: Risk-Neutral Density vs Lognormal Benchmark\n# =============================================================================\n\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\n        \"Risk-Neutral PDF (Probability Density)\",\n        \"Risk-Neutral CDF (Cumulative Probability)\",\n        \"PDF Difference (Market-Implied minus Lognormal)\",\n        \"Log-Return Density\"\n    ),\n    vertical_spacing=0.12,\n    horizontal_spacing=0.08\n)\n\n# --- (1) PDF comparison ---\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=rn_pdf_norm,\n    mode='lines', name='Market-Implied RND',\n    line=dict(color='#EF553B', width=2.5),\n    legendgroup='pdf'\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=lognorm_pdf,\n    mode='lines', name='Lognormal (BS)',\n    line=dict(color='#636EFA', width=2, dash='dash'),\n    legendgroup='pdf'\n), row=1, col=1)\n\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"gray\", row=1, col=1)\n\n# --- (2) CDF comparison ---\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=rn_cdf,\n    mode='lines', name='Market CDF',\n    line=dict(color='#EF553B', width=2.5),\n    legendgroup='cdf', showlegend=False\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=lognorm_cdf,\n    mode='lines', name='Lognormal CDF',\n    line=dict(color='#636EFA', width=2, dash='dash'),\n    legendgroup='cdf', showlegend=False\n), row=1, col=2)\n\n# --- (3) PDF difference (where does the market disagree with BS?) ---\npdf_diff = rn_pdf_norm - lognorm_pdf\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=pdf_diff,\n    mode='lines', name='Density Difference',\n    line=dict(color='#AB63FA', width=2),\n    fill='tozeroy',\n    fillcolor='rgba(171, 99, 250, 0.15)'\n), row=2, col=1)\n\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"gray\", row=2, col=1)\n\n# --- (4) Log-return density ---\n# Transform from price space to return space: R = ln(S_T / S_0)\nreturns = np.log(K_rnd / S0) * 100  # Percentage log returns\n\n# Transform PDF: f_R(r) = S_0 * exp(r) * f_S(S_0 * exp(r))\n# But since we already have pdf on K grid, just replot vs returns\nfig.add_trace(go.Scatter(\n    x=returns, y=rn_pdf_norm * K_rnd,  # Jacobian: dK = K * dR\n    mode='lines', name='Market (Returns)',\n    line=dict(color='#EF553B', width=2.5),\n    showlegend=False\n), row=2, col=2)\n\nfig.add_trace(go.Scatter(\n    x=returns, y=lognorm_pdf * K_rnd,\n    mode='lines', name='Lognormal (Returns)',\n    line=dict(color='#636EFA', width=2, dash='dash'),\n    showlegend=False\n), row=2, col=2)\n\nfig.add_vline(x=0, line_dash=\"dot\", line_color=\"gray\", row=2, col=2)\n\n# Layout\nfig.update_xaxes(title_text=\"Strike Price ($)\", row=1, col=1)\nfig.update_xaxes(title_text=\"Strike Price ($)\", row=1, col=2)\nfig.update_xaxes(title_text=\"Strike Price ($)\", row=2, col=1)\nfig.update_xaxes(title_text=\"Log Return (%)\", row=2, col=2)\nfig.update_yaxes(title_text=\"Density\", row=1, col=1)\nfig.update_yaxes(title_text=\"Probability\", row=1, col=2)\nfig.update_yaxes(title_text=\"Density Diff\", row=2, col=1)\nfig.update_yaxes(title_text=\"Density\", row=2, col=2)\n\nfig.update_layout(\n    title=f\"Breeden-Litzenberger Risk-Neutral Distribution \u2014 {TICKER} ({selected_expiry}, {actual_dte} DTE)\",\n    template=\"plotly_white\",\n    width=1100, height=800,\n    showlegend=True,\n    legend=dict(x=0.02, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hqz55ozok2b",
   "source": "## Step 7: Statistical Analysis \u2014 Moments, Tail Risk, and Skewness\n\nWe compute the first four moments of the market-implied distribution and compare them \nto the lognormal benchmark. Key metrics:\n- **Mean**: Expected future price under the risk-neutral measure (should match the forward price)\n- **Variance/Std**: Width of the distribution\n- **Skewness**: Asymmetry \u2014 negative skew means the market prices in larger downside moves\n- **Excess Kurtosis**: Fat tails \u2014 positive means the market expects more extreme moves than lognormal\n- **Tail probabilities**: Probability of large declines (e.g., -10%, -20%)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kqz4oycld2n",
   "source": "def compute_moments(K, pdf_vals):\n    \"\"\"Compute moments of a distribution given on a discrete grid.\"\"\"\n    total = trapz(pdf_vals, K)\n    f = pdf_vals / total\n    \n    mean = trapz(K * f, K)\n    var = trapz((K - mean)**2 * f, K)\n    std = np.sqrt(var)\n    skew = trapz(((K - mean) / std)**3 * f, K)\n    kurt = trapz(((K - mean) / std)**4 * f, K) - 3.0\n    \n    return {'mean': mean, 'std': std, 'variance': var, 'skewness': skew, 'excess_kurtosis': kurt}\n\n# Compute moments for both distributions\nmarket_moments = compute_moments(K_rnd, rn_pdf_norm)\nlognorm_moments = compute_moments(K_rnd, lognorm_pdf)\n\n# --- Display comparison table ---\nprint(\"=\" * 70)\nprint(f\"  DISTRIBUTION MOMENTS COMPARISON \u2014 {TICKER} ({selected_expiry})\")\nprint(\"=\" * 70)\nprint(f\"{'Statistic':<25} {'Market-Implied':>18} {'Lognormal (BS)':>18}\")\nprint(\"-\" * 70)\nprint(f\"{'Mean E[S_T]':<25} ${market_moments['mean']:>17.2f} ${lognorm_moments['mean']:>17.2f}\")\nprint(f\"{'Std Dev':<25} ${market_moments['std']:>17.2f} ${lognorm_moments['std']:>17.2f}\")\nprint(f\"{'Skewness':<25} {market_moments['skewness']:>18.4f} {lognorm_moments['skewness']:>18.4f}\")\nprint(f\"{'Excess Kurtosis':<25} {market_moments['excess_kurtosis']:>18.4f} {lognorm_moments['excess_kurtosis']:>18.4f}\")\nprint(f\"{'Forward Price':<25} ${F:>17.2f}\")\nprint(\"-\" * 70)\n\n# --- Tail probabilities ---\nprint(f\"\\n{'TAIL RISK ANALYSIS':^70}\")\nprint(\"=\" * 70)\nprint(f\"{'Event':<35} {'Market Prob':>15} {'Lognormal Prob':>15}\")\nprint(\"-\" * 70)\n\nfor pct_decline in [-5, -10, -15, -20, -25]:\n    K_level = S0 * (1 + pct_decline / 100)\n    mask = K_rnd <= K_level\n    mkt_prob = trapz(rn_pdf_norm[mask], K_rnd[mask]) if mask.any() else 0.0\n    ln_prob = norm.cdf((np.log(K_level) - mu_ln) / sigma_ln)\n    label = f\"Decline >= {abs(pct_decline)}% (S_T < ${K_level:.0f})\"\n    print(f\"{label:<35} {mkt_prob:>14.4%} {ln_prob:>14.4%}\")\n\nprint()\nfor pct_gain in [5, 10, 15, 20]:\n    K_level = S0 * (1 + pct_gain / 100)\n    mask = K_rnd >= K_level\n    mkt_prob = trapz(rn_pdf_norm[mask], K_rnd[mask]) if mask.any() else 0.0\n    ln_prob = 1.0 - norm.cdf((np.log(K_level) - mu_ln) / sigma_ln)\n    label = f\"Gain >= {pct_gain}% (S_T > ${K_level:.0f})\"\n    print(f\"{label:<35} {mkt_prob:>14.4%} {ln_prob:>14.4%}\")\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nds5sq393se",
   "source": "## Step 8: Sensitivity to the Smoothing Parameter $\\Delta$\n\nThe finite difference step size $\\Delta$ trades off resolution vs smoothness (Malz 2014, Sec. 2.4). \nMalz recommends $\\Delta = \\alpha \\cdot F$ with $\\alpha \\approx 0.025$ as a baseline. \nWe show how the extracted density changes for different values of $\\alpha \\in \\{0.01, 0.025, 0.05, 0.10\\}$.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g6zia58syqp",
   "source": "# --- Sensitivity to Delta ---\nfig = go.Figure()\n\ndelta_fracs = [0.010, 0.025, 0.050, 0.100]\ncolors = ['#FF6692', '#EF553B', '#636EFA', '#00CC96']\n\nfor delta_frac, color in zip(delta_fracs, colors):\n    _, _, pdf_test = breeden_litzenberger(K_rnd, delta_frac=delta_frac)\n    pdf_test = np.maximum(pdf_test, 0)\n    pdf_test = pdf_test / trapz(pdf_test, K_rnd)\n    \n    fig.add_trace(go.Scatter(\n        x=K_rnd, y=pdf_test,\n        mode='lines',\n        name=f'Delta = {delta_frac:.3f} (${delta_frac * F:.1f})',\n        line=dict(color=color, width=2)\n    ))\n\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=lognorm_pdf,\n    mode='lines', name='Lognormal (BS)',\n    line=dict(color='gray', width=1.5, dash='dash')\n))\n\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"lightgray\")\nfig.update_layout(\n    title=f\"Sensitivity of Risk-Neutral Density to Step Size Delta\",\n    xaxis_title=\"Strike Price ($)\",\n    yaxis_title=\"Density\",\n    template=\"plotly_white\",\n    width=900, height=500,\n    legend=dict(x=0.70, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bv0hr36vwxs",
   "source": "## Step 9: Tail Risk Visualisation\n\nHighlight where the market prices significantly more or less probability than the \nlognormal model. The **left tail** (large declines) is typically heavier in market-implied \ndistributions \u2014 this is the **volatility skew** in action.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0bjckboi8rqf",
   "source": "# --- Tail risk: shaded area comparison ---\nfig = go.Figure()\n\n# Full distributions\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=rn_pdf_norm,\n    mode='lines', name='Market-Implied',\n    line=dict(color='#EF553B', width=2.5)\n))\n\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=lognorm_pdf,\n    mode='lines', name='Lognormal (BS)',\n    line=dict(color='#636EFA', width=2, dash='dash')\n))\n\n# Shade left tail (decline > 10%)\nK_left_tail = S0 * 0.90\nleft_mask = K_rnd <= K_left_tail\nif left_mask.any():\n    fig.add_trace(go.Scatter(\n        x=K_rnd[left_mask], y=rn_pdf_norm[left_mask],\n        fill='tozeroy', fillcolor='rgba(239, 85, 59, 0.3)',\n        line=dict(width=0), showlegend=True,\n        name=f'Market left tail (>{10}% decline)'\n    ))\n    fig.add_trace(go.Scatter(\n        x=K_rnd[left_mask], y=lognorm_pdf[left_mask],\n        fill='tozeroy', fillcolor='rgba(99, 110, 250, 0.15)',\n        line=dict(width=0), showlegend=True,\n        name=f'Lognormal left tail'\n    ))\n\n# Shade right tail (gain > 10%)\nK_right_tail = S0 * 1.10\nright_mask = K_rnd >= K_right_tail\nif right_mask.any():\n    fig.add_trace(go.Scatter(\n        x=K_rnd[right_mask], y=rn_pdf_norm[right_mask],\n        fill='tozeroy', fillcolor='rgba(0, 204, 150, 0.3)',\n        line=dict(width=0), showlegend=True,\n        name=f'Market right tail (>{10}% gain)'\n    ))\n\nfig.add_vline(x=S0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Current Price\")\nfig.add_vline(x=K_left_tail, line_dash=\"dot\", line_color=\"#EF553B\", annotation_text=\"-10%\")\nfig.add_vline(x=K_right_tail, line_dash=\"dot\", line_color=\"#00CC96\", annotation_text=\"+10%\")\n\nfig.update_layout(\n    title=f\"Tail Risk: Market-Implied vs Lognormal \u2014 {TICKER}\",\n    xaxis_title=\"Strike Price ($)\",\n    yaxis_title=\"Density\",\n    template=\"plotly_white\",\n    width=1000, height=550,\n    legend=dict(x=0.65, y=0.98)\n)\nfig.show()\n\n# --- Print tail probability ratios ---\nprint(\"\\nTAIL PROBABILITY RATIOS (Market / Lognormal):\")\nprint(\"=\" * 50)\nfor pct in [5, 10, 15, 20]:\n    K_lev = S0 * (1 - pct / 100)\n    mask = K_rnd <= K_lev\n    if mask.any():\n        mkt_p = trapz(rn_pdf_norm[mask], K_rnd[mask])\n        ln_p = norm.cdf((np.log(K_lev) - mu_ln) / sigma_ln)\n        ratio = mkt_p / ln_p if ln_p > 0 else float('inf')\n        print(f\"  {pct}% decline: Market = {mkt_p:.4%}, Lognormal = {ln_p:.4%}, Ratio = {ratio:.2f}x\")\nprint(\"=\" * 50)\nprint(\"\\nRatios > 1 mean the market prices MORE probability of that tail event than BS assumes.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v0a814wf36",
   "source": "## Step 10: Diagnostics and Quality Checks\n\nFollowing **Malz (2014, Sec. 2.5)**, we verify our extraction is sensible:\n1. The CDF should range from ~0 to ~1 (valid probability distribution)\n2. The PDF should be non-negative everywhere (no butterfly arbitrage)\n3. The mean of the RND should approximate the forward price (no-arbitrage, see Malz 2014, Eq. 2\u20133)\n4. Exercise-price deltas at the extremes should be close to 0 and $-e^{-r\\tau}$ (Malz 2014, Sec. 2.5)\n5. Low vega at data boundaries ensures flat extrapolation has minimal impact",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rdxrq0zuqlp",
   "source": "# --- Diagnostics ---\nprint(\"DIAGNOSTIC CHECKS\")\nprint(\"=\" * 60)\n\n# 1. CDF bounds\nprint(f\"\\n1. CDF bounds:\")\nprint(f\"   CDF at lowest strike  (K={K_rnd[0]:.0f}):  {rn_cdf[0]:.6f}  (should be ~0)\")\nprint(f\"   CDF at highest strike (K={K_rnd[-1]:.0f}): {rn_cdf[-1]:.6f}  (should be ~1)\")\n\n# 2. PDF non-negativity\nneg_count = np.sum(rn_pdf < 0)\nneg_pct = neg_count / len(rn_pdf) * 100\nprint(f\"\\n2. PDF non-negativity:\")\nprint(f\"   Negative density points: {neg_count}/{len(rn_pdf)} ({neg_pct:.1f}%)\")\nif neg_count > 0:\n    print(f\"   Max negative density: {rn_pdf.min():.8f}\")\n    print(f\"   (These are set to 0 in the normalised PDF)\")\n\n# 3. Mean vs forward price\nmean_err = abs(market_moments['mean'] - F) / F * 100\nprint(f\"\\n3. No-arbitrage mean check:\")\nprint(f\"   RND mean E[S_T]:  ${market_moments['mean']:.2f}\")\nprint(f\"   Forward price F:  ${F:.2f}\")\nprint(f\"   Error:            {mean_err:.4f}%\")\n\n# 4. Exercise-price deltas at data boundaries\n# Delta of call at lowest observed strike (should be near -1, i.e., deep ITM call)\nd1_low = bs_d1(S0, K_min_data, T, r, q, sigma_interp(np.array([K_min_data]))[0])\ndelta_low = -np.exp(-r * T) * norm.cdf(-d1_low + sigma_interp(np.array([K_min_data]))[0] * np.sqrt(T))\n# Simpler: exercise-price delta \u2248 \u2202c/\u2202X\neps = 0.01\ndelta_ex_low = (call_value(np.array([K_min_data + eps])) - call_value(np.array([K_min_data - eps]))) / (2 * eps)\ndelta_ex_high = (call_value(np.array([K_max_data + eps])) - call_value(np.array([K_max_data - eps]))) / (2 * eps)\n\nprint(f\"\\n4. Exercise-price deltas at data boundaries:\")\nprint(f\"   \u2202c/\u2202X at K_min (${K_min_data:.0f}): {delta_ex_low[0]:.4f}  (should be near -e^{{-r\u03c4}} = {-np.exp(-r*T):.4f})\")\nprint(f\"   \u2202c/\u2202X at K_max (${K_max_data:.0f}): {delta_ex_high[0]:.4f}  (should be near 0)\")\n\n# 5. Vega at boundaries (low vega = extrapolation has less impact)\nvega_min = bs_vega(S0, K_min_data, T, r, q, sigma_interp(np.array([K_min_data]))[0])\nvega_max = bs_vega(S0, K_max_data, T, r, q, sigma_interp(np.array([K_max_data]))[0])\nvega_atm = bs_vega(S0, S0, T, r, q, sigma_atm)\n\nprint(f\"\\n5. Vega ratios (boundary vega / ATM vega):\")\nprint(f\"   Vega(K_min)/Vega(ATM): {vega_min/vega_atm:.4f}  (should be small)\")\nprint(f\"   Vega(K_max)/Vega(ATM): {vega_max/vega_atm:.4f}  (should be small)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"All diagnostics complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8lznrpkzag6",
   "source": "## Step 11: Multi-Expiry Analysis \u2014 3D Implied Volatility Surface\n\nThe volatility smile varies with time to expiry, forming the **implied volatility surface** $\\sigma(X, \\tau)$.\nThis is a key object in quantitative finance \u2014 it encodes the market's full forward-looking view across\nboth strike and maturity dimensions.\n\nWe fetch multiple expiries (7\u2013180 DTE), compute implied volatilities for each, and plot the surface\nin 3D using Plotly. The surface is parameterised by **moneyness** $K/S$ (x-axis) and\n**time to expiry** in days (y-axis), with implied volatility on the z-axis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "77pif18tesj",
   "source": "# --- Fetch multiple expiries and build vol surface data ---\nMIN_DTE, MAX_DTE = 7, 180\n\nexpiry_candidates = []\nfor exp_str in expiry_dates:\n    exp_date = datetime.strptime(exp_str, \"%Y-%m-%d\")\n    dte = (exp_date - today).days\n    if MIN_DTE <= dte <= MAX_DTE:\n        expiry_candidates.append((exp_str, dte))\n\n# Take up to 6 roughly evenly spaced expiries\nif len(expiry_candidates) > 6:\n    step = len(expiry_candidates) // 6\n    expiry_sel = [expiry_candidates[i] for i in range(0, len(expiry_candidates), step)][:6]\nelse:\n    expiry_sel = expiry_candidates\n\nprint(f\"Selected {len(expiry_sel)} expiries for surface construction:\")\nfor exp_str, dte in expiry_sel:\n    print(f\"  {exp_str}  ({dte} DTE)\")\n\ndef make_sigma_interp(smile_df):\n    \"\"\"Factory: build a clamped cubic spline sigma(K) from a smile DataFrame.\"\"\"\n    Ks = smile_df['strike'].values\n    ivs = smile_df['iv'].values\n    spline = CubicSpline(Ks, ivs, bc_type=((1, 0.0), (1, 0.0)))\n    K_lo, K_hi = Ks[0], Ks[-1]\n    iv_lo, iv_hi = ivs[0], ivs[-1]\n    def _sigma(K):\n        K = np.atleast_1d(K)\n        out = np.empty_like(K, dtype=float)\n        out[K < K_lo] = iv_lo\n        out[K > K_hi] = iv_hi\n        mask = (K >= K_lo) & (K <= K_hi)\n        out[mask] = spline(K[mask])\n        return np.maximum(out, 0.001)\n    return _sigma\n\n# Build vol surface data: list of (dte, moneyness_grid, iv_grid, smile_raw)\nsurface_data = []\n\nfor exp_str, dte in expiry_sel:\n    T_exp = dte / 365.0\n    chain_exp = spy.option_chain(exp_str)\n\n    calls_exp = clean_options(chain_exp.calls, 'call', S0)\n    puts_exp = clean_options(chain_exp.puts, 'put', S0)\n\n    # Compute IVs\n    c_ivs = [implied_vol_call(row['mid'], S0, row['strike'], T_exp, r, q)\n             for _, row in calls_exp.iterrows()]\n    calls_exp = calls_exp.copy(); calls_exp['iv_computed'] = c_ivs\n    p_ivs = [implied_vol_put(row['mid'], S0, row['strike'], T_exp, r, q)\n             for _, row in puts_exp.iterrows()]\n    puts_exp = puts_exp.copy(); puts_exp['iv_computed'] = p_ivs\n    calls_exp = calls_exp.dropna(subset=['iv_computed'])\n    puts_exp = puts_exp.dropna(subset=['iv_computed'])\n\n    # Build OTM smile\n    otm_p = puts_exp[puts_exp['strike'] < S0][['strike','iv_computed','moneyness']].rename(columns={'iv_computed':'iv'})\n    otm_c = calls_exp[calls_exp['strike'] >= S0][['strike','iv_computed','moneyness']].rename(columns={'iv_computed':'iv'})\n    smile_exp = pd.concat([otm_p, otm_c]).sort_values('strike').drop_duplicates('strike')\n    smile_exp = smile_exp[(smile_exp['iv'] > 0.01) & (smile_exp['iv'] < 1.5)]\n\n    if len(smile_exp) < 5:\n        continue\n\n    sig_fn = make_sigma_interp(smile_exp)\n\n    # Evaluate on common moneyness grid\n    m_grid = np.linspace(0.80, 1.20, 200)\n    K_grid = m_grid * S0\n    iv_grid = sig_fn(K_grid)\n\n    surface_data.append({\n        'expiry': exp_str, 'dte': dte, 'T': T_exp,\n        'moneyness': m_grid, 'iv': iv_grid,\n        'sigma_fn': sig_fn, 'smile_raw': smile_exp\n    })\n\nprint(f\"\\nBuilt surface data for {len(surface_data)} expiries.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7o4ekfbeutd",
   "source": "# --- 3D Implied Volatility Surface ---\nm_common = np.linspace(0.80, 1.20, 200)\ndte_vals = np.array([d['dte'] for d in surface_data])\niv_matrix = np.array([d['iv'] * 100 for d in surface_data])  # shape: (n_expiries, n_moneyness)\n\nfig = go.Figure()\n\n# Surface\nfig.add_trace(go.Surface(\n    x=m_common, y=dte_vals, z=iv_matrix,\n    colorscale='Viridis', opacity=0.85,\n    colorbar=dict(title='IV (%)'),\n    name='IV Surface'\n))\n\n# Overlay raw data points\nfor d in surface_data:\n    raw = d['smile_raw']\n    fig.add_trace(go.Scatter3d(\n        x=raw['moneyness'].values,\n        y=np.full(len(raw), d['dte']),\n        z=raw['iv'].values * 100,\n        mode='markers',\n        marker=dict(size=2.5, color='red'),\n        name=f\"{d['expiry']}\",\n        showlegend=False\n    ))\n\nfig.update_layout(\n    title=f'3D Implied Volatility Surface \u2014 {TICKER}',\n    scene=dict(\n        xaxis_title='Moneyness (K/S)',\n        yaxis_title='Days to Expiry',\n        zaxis_title='Implied Volatility (%)',\n        camera=dict(eye=dict(x=1.8, y=-1.8, z=1.0))\n    ),\n    template='plotly_white',\n    width=1000, height=700\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y5tfxklv6ge",
   "source": "## Step 12: 3D Risk-Neutral Density Surface\n\nApplying the Breeden-Litzenberger extraction (BL 1978, Eq. 2) to each expiry, we obtain the\n**risk-neutral density surface** $\\tilde{\\pi}(K/S, \\tau)$. This shows how the market-implied\ndistribution evolves with maturity:\n- Short-dated distributions are narrow and peaked (low total variance)\n- Longer-dated distributions spread out and typically show more pronounced skewness",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pn62bl7ygr8",
   "source": "def breeden_litzenberger_general(K_grid, S, T_exp, r_val, q_val, sigma_fn, delta_frac=0.025):\n    \"\"\"\n    Generalised BL extraction for any expiry.\n    \n    Parameters:\n        K_grid   : strike grid\n        S        : spot price\n        T_exp    : time to expiry (years)\n        r_val    : risk-free rate\n        q_val    : dividend yield\n        sigma_fn : interpolated vol function sigma(K)\n        delta_frac : step size as fraction of forward\n    \"\"\"\n    F_exp = S * np.exp((r_val - q_val) * T_exp)\n    Delta = delta_frac * F_exp\n    discount = np.exp(r_val * T_exp)\n    \n    def _c(K):\n        return bs_call_price(S, np.atleast_1d(K), T_exp, r_val, q_val, sigma_fn(np.atleast_1d(K)))\n    \n    c_up = _c(K_grid + Delta)\n    c_dn = _c(K_grid - Delta)\n    c_ctr = _c(K_grid)\n    \n    pdf = discount * (c_up + c_dn - 2.0 * c_ctr) / (Delta**2)\n    pdf = np.maximum(pdf, 0)\n    total = trapz(pdf, K_grid)\n    if total > 0:\n        pdf = pdf / total\n    return pdf\n\n# --- Compute RND for each expiry on moneyness grid ---\nm_rnd = np.linspace(0.80, 1.20, 300)\nK_rnd_grid = m_rnd * S0\nrnd_matrix = []\n\nfor d in surface_data:\n    pdf_exp = breeden_litzenberger_general(\n        K_rnd_grid, S0, d['T'], r, q, d['sigma_fn']\n    )\n    # Convert from price-space density to moneyness-space: f_m = f_K * S0\n    rnd_matrix.append(pdf_exp * S0)\n\nrnd_matrix = np.array(rnd_matrix)\n\n# --- 3D RND Surface ---\nfig = go.Figure()\n\nfig.add_trace(go.Surface(\n    x=m_rnd, y=dte_vals, z=rnd_matrix,\n    colorscale='RdBu_r', opacity=0.85,\n    colorbar=dict(title='Density'),\n    name='RND Surface'\n))\n\nfig.update_layout(\n    title=f'3D Risk-Neutral Density Surface \u2014 {TICKER}',\n    scene=dict(\n        xaxis_title='Moneyness (K/S)',\n        yaxis_title='Days to Expiry',\n        zaxis_title='Probability Density',\n        camera=dict(eye=dict(x=1.8, y=-1.8, z=1.0))\n    ),\n    template='plotly_white',\n    width=1000, height=700\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ms6uipm3dn",
   "source": "## Step 13: Term Structure of Risk-Neutral Moments\n\nHow do the moments of the risk-neutral distribution evolve with maturity? We compute\nskewness, excess kurtosis, standard deviation, and left-tail probability ($P(S_T < 0.9 S_0)$)\nfor each expiry and compare against the lognormal benchmark. This reveals whether\nthe market's departure from Black-Scholes is persistent or maturity-dependent.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m2ug085trig",
   "source": "# --- Term structure of moments ---\nK_moments = np.linspace(S0 * 0.70, S0 * 1.30, 1500)\nterm_moments_mkt = []\nterm_moments_ln = []\nterm_dtes = []\n\nfor d in surface_data:\n    # Market-implied RND\n    pdf_mkt = breeden_litzenberger_general(K_moments, S0, d['T'], r, q, d['sigma_fn'])\n    mkt_m = compute_moments(K_moments, pdf_mkt)\n    \n    # Lognormal benchmark (use ATM vol for this expiry)\n    sigma_atm_exp = float(d['sigma_fn'](np.array([S0]))[0])\n    mu_ln_exp = np.log(S0) + (r - q - 0.5 * sigma_atm_exp**2) * d['T']\n    sigma_ln_exp = sigma_atm_exp * np.sqrt(d['T'])\n    ln_pdf_exp = (1.0 / (K_moments * sigma_ln_exp * np.sqrt(2*np.pi))) * \\\n                 np.exp(-0.5 * ((np.log(K_moments) - mu_ln_exp) / sigma_ln_exp)**2)\n    ln_m = compute_moments(K_moments, ln_pdf_exp)\n    \n    # Left-tail probability: P(S_T < 0.9 * S0)\n    K_tail = S0 * 0.90\n    mask_tail = K_moments <= K_tail\n    mkt_m['left_tail_10'] = trapz(pdf_mkt[mask_tail], K_moments[mask_tail]) if mask_tail.any() else 0\n    ln_m['left_tail_10'] = norm.cdf((np.log(K_tail) - mu_ln_exp) / sigma_ln_exp)\n    \n    term_moments_mkt.append(mkt_m)\n    term_moments_ln.append(ln_m)\n    term_dtes.append(d['dte'])\n\nterm_dtes = np.array(term_dtes)\n\n# --- 2x2 Term structure plot ---\nfig = make_subplots(rows=2, cols=2, subplot_titles=(\n    'Standard Deviation', 'Skewness', 'Excess Kurtosis', 'Left Tail P(decline > 10%)'\n), vertical_spacing=0.12, horizontal_spacing=0.10)\n\nmetrics = [\n    ('std', 'Std Dev ($)'),\n    ('skewness', 'Skewness'),\n    ('excess_kurtosis', 'Excess Kurtosis'),\n    ('left_tail_10', 'Probability')\n]\n\nfor i, (key, ylabel) in enumerate(metrics):\n    row, col = divmod(i, 2)\n    row += 1; col += 1\n    mkt_vals = [m[key] for m in term_moments_mkt]\n    ln_vals = [m[key] for m in term_moments_ln]\n    \n    fig.add_trace(go.Scatter(\n        x=term_dtes, y=mkt_vals, mode='lines+markers',\n        name='Market' if i == 0 else None, showlegend=(i == 0),\n        line=dict(color='#EF553B', width=2), marker=dict(size=7),\n        legendgroup='mkt'\n    ), row=row, col=col)\n    fig.add_trace(go.Scatter(\n        x=term_dtes, y=ln_vals, mode='lines+markers',\n        name='Lognormal' if i == 0 else None, showlegend=(i == 0),\n        line=dict(color='#636EFA', width=2, dash='dash'), marker=dict(size=7),\n        legendgroup='ln'\n    ), row=row, col=col)\n    fig.update_yaxes(title_text=ylabel, row=row, col=col)\n    fig.update_xaxes(title_text='DTE', row=row, col=col)\n\nfig.update_layout(\n    title=f'Term Structure of Risk-Neutral Moments \u2014 {TICKER}',\n    template='plotly_white', width=1050, height=700,\n    legend=dict(x=0.02, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lzj1v0dm0wb",
   "source": "## Step 14: Mixture of Lognormals (Jackwerth 2004, Exhibit 1)\n\nA **mixture of two lognormals** is a simple parametric model for the risk-neutral density\n(Jackwerth 2004, Sec. 2.1, Exhibit 1). The call price under a 2-component mixture is:\n\n$$c_{\\text{mix}}(K) = w \\cdot \\text{BS}(K, \\sigma_1) + (1-w) \\cdot \\text{BS}(K, \\sigma_2)$$\n\nwhere $w \\in [0,1]$ is the mixing weight and $\\sigma_1, \\sigma_2$ are two volatility levels.\nThis allows the model to capture skew and fat tails that a single lognormal cannot.\n\nWe fit the three parameters $(w, \\sigma_1, \\sigma_2)$ by minimising the sum of squared \nimplied volatility errors against the market smile.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lsqdxrryy0f",
   "source": "from scipy.optimize import minimize\n\ndef mixture_call_price(K, S, T_val, r_val, q_val, w, sigma1, sigma2):\n    \"\"\"Call price under a 2-component lognormal mixture.\"\"\"\n    return w * bs_call_price(S, K, T_val, r_val, q_val, sigma1) + \\\n           (1 - w) * bs_call_price(S, K, T_val, r_val, q_val, sigma2)\n\ndef mixture_implied_vol(K_arr, S, T_val, r_val, q_val, w, sigma1, sigma2):\n    \"\"\"Compute the implied vol of the mixture model at each strike.\"\"\"\n    mix_prices = mixture_call_price(K_arr, S, T_val, r_val, q_val, w, sigma1, sigma2)\n    ivs = []\n    for K_i, p_i in zip(K_arr, mix_prices):\n        iv = implied_vol_call(p_i, S, K_i, T_val, r_val, q_val)\n        ivs.append(iv if iv is not None else np.nan)\n    return np.array(ivs)\n\n# Fit to the single-expiry smile data\nK_smile = smile['strike'].values\niv_smile = smile['iv'].values\n\ndef objective(params):\n    w, s1, s2 = params\n    iv_model = mixture_implied_vol(K_smile, S0, T, r, q, w, s1, s2)\n    valid = ~np.isnan(iv_model)\n    if valid.sum() < 3:\n        return 1e6\n    return np.sum((iv_model[valid] - iv_smile[valid])**2)\n\n# Initial guess and bounds\nx0 = [0.7, sigma_atm * 0.8, sigma_atm * 1.5]\nbounds = [(0.01, 0.99), (0.01, 2.0), (0.01, 2.0)]\n\nresult = minimize(objective, x0, method='L-BFGS-B', bounds=bounds)\nw_fit, sigma1_fit, sigma2_fit = result.x\nprint(f\"Mixture fit: w={w_fit:.4f}, sigma1={sigma1_fit:.4f}, sigma2={sigma2_fit:.4f}\")\nprint(f\"Residual SSE: {result.fun:.8f}\")\n\n# --- Compute mixture density analytically ---\n# Mixture PDF = w * lognormal(sigma1) + (1-w) * lognormal(sigma2)\nmu1 = np.log(S0) + (r - q - 0.5 * sigma1_fit**2) * T\ns1 = sigma1_fit * np.sqrt(T)\nmu2 = np.log(S0) + (r - q - 0.5 * sigma2_fit**2) * T\ns2 = sigma2_fit * np.sqrt(T)\n\nln_pdf1 = (1.0 / (K_rnd * s1 * np.sqrt(2*np.pi))) * np.exp(-0.5*((np.log(K_rnd)-mu1)/s1)**2)\nln_pdf2 = (1.0 / (K_rnd * s2 * np.sqrt(2*np.pi))) * np.exp(-0.5*((np.log(K_rnd)-mu2)/s2)**2)\nmixture_pdf = w_fit * ln_pdf1 + (1 - w_fit) * ln_pdf2\n\n# --- Plot: BL-extracted vs mixture vs single lognormal ---\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=K_rnd, y=rn_pdf_norm, mode='lines', name='BL-Extracted RND',\n                         line=dict(color='#EF553B', width=2.5)))\nfig.add_trace(go.Scatter(x=K_rnd, y=mixture_pdf, mode='lines', name=f'Mixture (w={w_fit:.2f})',\n                         line=dict(color='#00CC96', width=2)))\nfig.add_trace(go.Scatter(x=K_rnd, y=lognorm_pdf, mode='lines', name='Single Lognormal',\n                         line=dict(color='#636EFA', width=2, dash='dash')))\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"gray\")\nfig.update_layout(\n    title=f'Mixture of Lognormals vs BL-Extracted RND \u2014 {TICKER} ({selected_expiry})',\n    xaxis_title='Strike Price ($)', yaxis_title='Density',\n    template='plotly_white', width=950, height=500,\n    legend=dict(x=0.65, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ydvl5h6b44",
   "source": "## Step 15: State Prices and Arrow-Debreu Securities (BL 1978, Theorem 1; Jackwerth 2004, Eq. 7)\n\nThe **state price density** $p(X)$ is the price today of a claim that pays \\$1 if $S_T = X$.\nIt is related to the risk-neutral density by (Jackwerth 2004, Eq. 7):\n\n$$p(X) = e^{-rT} \\cdot \\tilde{\\pi}(X)$$\n\nThe integral of state prices over all states must equal the **discount factor** (price of a \nrisk-free zero-coupon bond):\n\n$$\\int_0^\\infty p(X)\\, dX = e^{-rT}$$\n\nWe also show the **butterfly spread decomposition**: a butterfly centred at strike $X$ with\nwidth $\\Delta$ approximates an Arrow-Debreu security paying $\\Delta$ units at $S_T = X$\n(Breeden & Litzenberger 1978, Theorem 1).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a1uyvij0fhr",
   "source": "# --- State price density ---\nstate_price_density = np.exp(-r * T) * rn_pdf_norm\n\n# Verify integral equals discount factor\nsp_integral = trapz(state_price_density, K_rnd)\ndiscount_factor = np.exp(-r * T)\nprint(f\"Integral of state price density: {sp_integral:.6f}\")\nprint(f\"Discount factor e^(-rT):         {discount_factor:.6f}\")\nprint(f\"Error:                           {abs(sp_integral - discount_factor):.8f}\")\n\n# --- Butterfly spread decomposition ---\n# Show discrete butterfly prices at selected strikes as Arrow-Debreu approximations\nDelta_bf = 0.025 * F\nbf_strikes = np.arange(S0 * 0.85, S0 * 1.15, Delta_bf)\nbf_prices = []\nfor K_bf in bf_strikes:\n    c_up = call_value(np.array([K_bf + Delta_bf]))[0]\n    c_dn = call_value(np.array([K_bf - Delta_bf]))[0]\n    c_ctr = call_value(np.array([K_bf]))[0]\n    bf_price = (c_up + c_dn - 2 * c_ctr)  # butterfly spread cost\n    bf_prices.append(bf_price)\nbf_prices = np.array(bf_prices)\n\n# Butterfly price / Delta^2 should approximate state price density\nbf_density_approx = bf_prices / (Delta_bf**2)\n\n# --- Plot ---\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\n    'State Price Density p(X)', 'Butterfly Spread Decomposition'\n), horizontal_spacing=0.10)\n\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=state_price_density, mode='lines',\n    name='State Price Density', line=dict(color='#AB63FA', width=2.5)\n), row=1, col=1)\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"gray\", row=1, col=1)\nfig.update_yaxes(title_text='State Price Density', row=1, col=1)\nfig.update_xaxes(title_text='Strike ($)', row=1, col=1)\n\n# Butterfly approx vs continuous\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=state_price_density, mode='lines',\n    name='Continuous (BL)', line=dict(color='#AB63FA', width=2), showlegend=True\n), row=1, col=2)\nfig.add_trace(go.Bar(\n    x=bf_strikes, y=bf_density_approx, width=Delta_bf * 0.8,\n    name='Butterfly Approximation', marker_color='rgba(99,110,250,0.5)'\n), row=1, col=2)\nfig.update_yaxes(title_text='Density', row=1, col=2)\nfig.update_xaxes(title_text='Strike ($)', row=1, col=2)\n\nfig.update_layout(\n    title=f'State Prices & Arrow-Debreu Securities \u2014 {TICKER} ({selected_expiry})',\n    template='plotly_white', width=1100, height=450\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "iai6hx0z2l",
   "source": "## Step 16: No-Arbitrage Smile Slope Bounds (Malz 2014, Eq. 7\u20138)\n\nThe volatility smile must satisfy slope constraints to avoid arbitrage. Using the chain rule\non the call price $c(X) = v[S, X, T, \\sigma(X), r, q]$:\n\n$$\\frac{\\partial c}{\\partial X} = v_X + v_\\sigma \\cdot \\frac{d\\sigma}{dX}$$\n\n**Upper bound** (monotonicity: $\\partial c/\\partial X \\leq 0$, Malz 2014, Eq. 7):\n\n$$\\frac{d\\sigma}{dX} \\leq -\\frac{v_X}{v_\\sigma}$$\n\n**Lower bound** (bounded slope: $\\partial c/\\partial X \\geq -e^{-rT}$, Malz 2014, Eq. 8):\n\n$$\\frac{d\\sigma}{dX} \\geq -\\frac{v_X + e^{-rT}}{v_\\sigma}$$\n\nViolations of the upper bound produce negative densities; violations of the lower bound\nproduce CDF values outside $[0, 1]$.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ez4s38cvrl",
   "source": "# --- No-arbitrage smile slope bounds (Malz 2014, Eq. 7-8) ---\nK_bounds = np.linspace(S0 * 0.82, S0 * 1.18, 500)\nsigma_vals = sigma_interp(K_bounds)\n\n# v_X: partial derivative of BS call w.r.t. strike (exercise-price delta)\nd2_vals = bs_d2(S0, K_bounds, T, r, q, sigma_vals)\nv_X = -np.exp(-r * T) * norm.cdf(d2_vals)\n\n# v_sigma: vega\nv_sigma = bs_vega(S0, K_bounds, T, r, q, sigma_vals)\n\n# Actual smile slope (numerical derivative of spline)\ndK = K_bounds[1] - K_bounds[0]\nsigma_slope = np.gradient(sigma_vals, dK)\n\n# Bounds\nupper_bound = -v_X / np.where(v_sigma > 1e-10, v_sigma, np.inf)\nlower_bound = -(v_X + np.exp(-r * T)) / np.where(v_sigma > 1e-10, v_sigma, np.inf)\n\n# Count violations\nupper_violations = np.sum(sigma_slope > upper_bound)\nlower_violations = np.sum(sigma_slope < lower_bound)\nprint(f\"Upper bound violations: {upper_violations}/{len(K_bounds)}\")\nprint(f\"Lower bound violations: {lower_violations}/{len(K_bounds)}\")\n\n# --- Plot ---\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=K_bounds, y=sigma_slope, mode='lines',\n    name='Actual Smile Slope', line=dict(color='#EF553B', width=2.5)\n))\nfig.add_trace(go.Scatter(\n    x=K_bounds, y=upper_bound, mode='lines',\n    name='Upper Bound (Malz Eq. 7)', line=dict(color='#636EFA', width=1.5, dash='dash')\n))\nfig.add_trace(go.Scatter(\n    x=K_bounds, y=lower_bound, mode='lines',\n    name='Lower Bound (Malz Eq. 8)', line=dict(color='#00CC96', width=1.5, dash='dash')\n))\n\n# Shade no-arbitrage corridor\nfig.add_trace(go.Scatter(\n    x=np.concatenate([K_bounds, K_bounds[::-1]]),\n    y=np.concatenate([upper_bound, lower_bound[::-1]]),\n    fill='toself', fillcolor='rgba(99, 110, 250, 0.08)',\n    line=dict(width=0), showlegend=True, name='No-Arbitrage Corridor'\n))\n\nfig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\")\nfig.add_vline(x=S0, line_dash=\"dot\", line_color=\"gray\")\n\nfig.update_layout(\n    title=f'No-Arbitrage Smile Slope Bounds \u2014 {TICKER} ({selected_expiry})',\n    xaxis_title='Strike Price ($)', yaxis_title='d\u03c3/dX (per $)',\n    template='plotly_white', width=950, height=500,\n    legend=dict(x=0.60, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wwl32uqkknd",
   "source": "## Step 17: Pricing Kernel Extraction (Jackwerth 2004, Eq. 12, pp. 54\u201356)\n\nThe **pricing kernel** (stochastic discount factor) $m(S_T)$ connects the risk-neutral density\n$\\tilde{\\pi}$ to the physical (real-world) density $f^P$ (Jackwerth 2004, Eq. 12):\n\n$$m(S_T) = \\frac{\\tilde{\\pi}(S_T)}{e^{rT} \\cdot f^P(S_T)}$$\n\nUnder standard risk aversion, $m(S_T)$ should be **monotonically decreasing** in $S_T$:\ninvestors value an extra dollar more in bad states (low $S_T$) than in good states.\n\nJackwerth (2004, pp. 54\u201356) documents the **pricing kernel puzzle**: empirically, the pricing\nkernel estimated from S&P 500 options is *not* monotonically decreasing \u2014 it often increases\nover some range, inconsistent with a representative agent with concave utility.\n\nWe estimate the physical density $f^P$ from 3 years of historical SPY returns using a\nGaussian kernel density estimator.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3yai6pn9c0i",
   "source": "from scipy.stats import gaussian_kde\n\n# --- Fetch 3 years of historical SPY data ---\nhist_data = spy.history(period=\"3y\")\nhist_close = hist_data['Close'].values\n\n# Compute T-day rolling returns (matching option expiry horizon)\ntrading_days = max(int(actual_dte * 252 / 365), 1)\nhist_returns = hist_close[trading_days:] / hist_close[:-trading_days]  # S_{t+T} / S_t\n\n# Map to future price levels: S_T = S0 * return_ratio\nhist_S_T = S0 * hist_returns\n\nprint(f\"Historical return horizon: {trading_days} trading days (~{actual_dte} calendar days)\")\nprint(f\"Number of overlapping return observations: {len(hist_returns)}\")\nprint(f\"Historical return range: [{hist_returns.min():.4f}, {hist_returns.max():.4f}]\")\n\n# --- Estimate physical density via Gaussian KDE ---\nkde = gaussian_kde(hist_S_T, bw_method='silverman')\nphysical_pdf = kde(K_rnd)\n\n# Ensure physical density is positive (for division)\nphysical_pdf = np.maximum(physical_pdf, 1e-12)\n\n# --- Compute pricing kernel ---\n# m(S_T) = RND(S_T) / (e^{rT} * physical_density(S_T))\npricing_kernel = rn_pdf_norm / (np.exp(r * T) * physical_pdf)\n\n# Focus on region with sufficient data support (moneyness 0.85-1.15)\npk_mask = (K_rnd >= S0 * 0.85) & (K_rnd <= S0 * 1.15)\n\n# Check monotonicity\npk_trimmed = pricing_kernel[pk_mask]\nK_trimmed = K_rnd[pk_mask]\ndiffs = np.diff(pk_trimmed)\nn_increasing = np.sum(diffs > 0)\nprint(f\"\\nPricing kernel monotonicity check (0.85-1.15 moneyness):\")\nprint(f\"  Increasing segments: {n_increasing}/{len(diffs)} ({n_increasing/len(diffs)*100:.1f}%)\")\nprint(f\"  {'NOT monotonically decreasing (pricing kernel puzzle!)' if n_increasing > len(diffs)*0.1 else 'Approximately monotonically decreasing'}\")\n\n# --- 2-panel plot ---\nfig = make_subplots(rows=2, cols=1, subplot_titles=(\n    'Risk-Neutral vs Physical Density',\n    'Pricing Kernel m(S_T)'\n), vertical_spacing=0.12, row_heights=[0.45, 0.55])\n\n# Top: densities\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=rn_pdf_norm, mode='lines', name='Risk-Neutral (Q)',\n    line=dict(color='#EF553B', width=2.5)\n), row=1, col=1)\nfig.add_trace(go.Scatter(\n    x=K_rnd, y=physical_pdf, mode='lines', name='Physical (P, KDE)',\n    line=dict(color='#636EFA', width=2)\n), row=1, col=1)\nfig.add_vline(x=F, line_dash=\"dot\", line_color=\"gray\", row=1, col=1)\n\n# Bottom: pricing kernel\nfig.add_trace(go.Scatter(\n    x=K_rnd[pk_mask], y=pricing_kernel[pk_mask], mode='lines',\n    name='Pricing Kernel', line=dict(color='#AB63FA', width=2.5)\n), row=2, col=1)\nfig.add_hline(y=1.0, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\nfig.add_vline(x=S0, line_dash=\"dot\", line_color=\"gray\", row=2, col=1)\n\n# Add moneyness axis\nmoneyness_ticks = K_rnd[pk_mask] / S0\n\nfig.update_xaxes(title_text='Strike Price ($)', row=1, col=1)\nfig.update_xaxes(title_text='Strike Price ($)', row=2, col=1)\nfig.update_yaxes(title_text='Density', row=1, col=1)\nfig.update_yaxes(title_text='m(S_T)', row=2, col=1)\n\nfig.update_layout(\n    title=f'Pricing Kernel Extraction \u2014 {TICKER} ({selected_expiry}, Jackwerth 2004)',\n    template='plotly_white', width=950, height=750,\n    legend=dict(x=0.65, y=0.98)\n)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rbk9gboocsl",
   "source": "## References\n\n1. **Breeden, D.T. and Litzenberger, R.H.** (1978). \"Prices of State-Contingent Claims Implicit in Option Prices.\" *Journal of Business*, 51(4), 621\u2013651.\n   - **Theorem 1**: Call options on aggregate consumption span all Arrow-Debreu securities.\n   - **Eq. 2**: $\\tilde{\\pi}(X) = e^{rT} \\partial^2 c / \\partial X^2$ \u2014 the core BL result (Steps 5, 12, 15).\n   - **Eq. 5**: Explicit derivation that BL applied to BS recovers the lognormal density (Step 6).\n\n2. **Malz, A.M.** (2014). \"A Simple and Reliable Way to Compute Option-Based Risk-Neutral Distributions.\" *Fed. Reserve Bank of New York Staff Reports*, No. 677.\n   - **Eq. 2\u20133**: CDF and PDF via finite differences (Step 5).\n   - **Eq. 4**: Call valuation function from interpolated smile (Step 4).\n   - **Eq. 5\u20136**: Clamped cubic spline boundary conditions (Step 4).\n   - **Eq. 7\u20138**: No-arbitrage upper/lower bounds on smile slope (Step 16).\n   - **Sec. 2.4**: Smoothing parameter $\\Delta$ selection (Step 8).\n   - **Sec. 2.5**: Diagnostic checks \u2014 CDF bounds, mean check, exercise-price deltas (Step 10).\n\n3. **Jackwerth, J.C.** (2004). \"Option-Implied Risk-Neutral Distributions and Risk Aversion.\" *Research Foundation of CFA Institute*.\n   - **Exhibit 1**: Mixture of lognormals as parametric RND model (Step 14).\n   - **Eq. 7**: State price density $p(X) = e^{-rT} \\tilde{\\pi}(X)$ (Step 15).\n   - **Eq. 12**: Pricing kernel $m(S_T) = \\tilde{\\pi}(S_T) / [e^{rT} f^P(S_T)]$ (Step 17).\n   - **pp. 54\u201356**: The pricing kernel puzzle \u2014 non-monotonic pricing kernels (Step 17).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "r9vaf81r34h",
   "source": "## Summary: Static Publication-Quality Figure\n\nA single matplotlib figure summarising the full pipeline for export/presentation.\nExpanded to 3x2 to include the pricing kernel and mixture model comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p37i07fc03",
   "source": "fig, axes = plt.subplots(3, 2, figsize=(14, 15))\nfig.suptitle(f'Breeden-Litzenberger Risk-Neutral Distribution \u2014 {TICKER} ({selected_expiry}, {actual_dte} DTE)',\n             fontsize=14, fontweight='bold')\n\n# (1) Volatility Smile\nax = axes[0, 0]\nax.scatter(smile['strike'], smile['iv'] * 100, s=15, color='red', alpha=0.7, label='Market IV', zorder=5)\nax.plot(K_fine, sigma_fine * 100, 'b-', linewidth=1.5, label='Spline Fit')\nax.axvline(S0, color='gray', linestyle='--', alpha=0.5, label='Spot')\nax.set_xlabel('Strike ($)')\nax.set_ylabel('Implied Volatility (%)')\nax.set_title('Volatility Smile')\nax.legend(fontsize=8)\nax.set_xlim(S0 * 0.80, S0 * 1.20)\nax.grid(True, alpha=0.3)\n\n# (2) Call Valuation Function\nax = axes[0, 1]\nax.plot(K_fine, C_fine, 'b-', linewidth=1.5, label='c(K) from spline')\nax.scatter(calls['strike'], calls['mid'], s=12, color='red', alpha=0.5, label='Market calls', zorder=5)\nax.axvline(S0, color='gray', linestyle='--', alpha=0.5)\nax.set_xlabel('Strike ($)')\nax.set_ylabel('Call Price ($)')\nax.set_title('Call Valuation Function')\nax.legend(fontsize=8)\nax.set_xlim(S0 * 0.80, S0 * 1.20)\nax.grid(True, alpha=0.3)\n\n# (3) Risk-Neutral PDF\nax = axes[1, 0]\nax.plot(K_rnd, rn_pdf_norm, 'r-', linewidth=2, label='Market-Implied RND')\nax.plot(K_rnd, lognorm_pdf, 'b--', linewidth=1.5, label='Lognormal (BS)')\nax.fill_between(K_rnd, rn_pdf_norm, alpha=0.15, color='red')\nax.axvline(F, color='gray', linestyle=':', alpha=0.5, label=f'Forward ${F:.0f}')\nax.set_xlabel('Strike ($)')\nax.set_ylabel('Density')\nax.set_title('Risk-Neutral Probability Density')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\n# (4) Risk-Neutral CDF\nax = axes[1, 1]\nax.plot(K_rnd, rn_cdf, 'r-', linewidth=2, label='Market-Implied CDF')\nax.plot(K_rnd, lognorm_cdf, 'b--', linewidth=1.5, label='Lognormal CDF')\nax.axvline(F, color='gray', linestyle=':', alpha=0.5)\nax.axhline(0.5, color='lightgray', linestyle=':', alpha=0.5)\nax.set_xlabel('Strike ($)')\nax.set_ylabel('Cumulative Probability')\nax.set_title('Risk-Neutral CDF')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\n# (5) Pricing Kernel\nax = axes[2, 0]\nax.plot(K_rnd[pk_mask], pricing_kernel[pk_mask], '-', color='purple', linewidth=2, label='Pricing Kernel')\nax.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\nax.axvline(S0, color='gray', linestyle=':', alpha=0.5, label='Spot')\nax.set_xlabel('Strike ($)')\nax.set_ylabel('m(S_T)')\nax.set_title('Pricing Kernel (Jackwerth 2004)')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\n# (6) Mixture vs BL-Extracted\nax = axes[2, 1]\nax.plot(K_rnd, rn_pdf_norm, 'r-', linewidth=2, label='BL-Extracted')\nax.plot(K_rnd, mixture_pdf, '-', color='green', linewidth=1.5, label=f'Mixture (w={w_fit:.2f})')\nax.plot(K_rnd, lognorm_pdf, 'b--', linewidth=1.5, label='Single Lognormal')\nax.axvline(F, color='gray', linestyle=':', alpha=0.5)\nax.set_xlabel('Strike ($)')\nax.set_ylabel('Density')\nax.set_title('Mixture of Lognormals Comparison')\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../docs/notes/bl_summary.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Figure saved to docs/notes/bl_summary.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}